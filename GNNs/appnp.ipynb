{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "absolute-restaurant",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import math\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "from deeprobust.graph import utils\n",
    "from copy import deepcopy\n",
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from Preprocess.preprocessing import *\n",
    "import random\n",
    "\n",
    "class SparseDropout(nn.Module):\n",
    "    def __init__(self, p):\n",
    "        super(SparseDropout, self).__init__()\n",
    "        self.p = p\n",
    "\n",
    "    def forward(self, input):\n",
    "        input_coal = input.coalesce()\n",
    "        drop_val = F.dropout(input_coal._values(), self.p, self.training)\n",
    "        return torch.sparse.FloatTensor(input_coal._indices(), drop_val, input.shape)\n",
    "\n",
    "class MixedLinear(nn.Module):\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(MixedLinear, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight = nn.Parameter(torch.Tensor(in_features, out_features))\n",
    "        if bias:\n",
    "            self.bias = nn.Parameter(torch.Tensor(out_features))\n",
    "        else:\n",
    "            self.register_parameter('bias', None)\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        # Our fan_in is interpreted by PyTorch as fan_out (swapped dimensions)\n",
    "        nn.init.kaiming_uniform_(self.weight, mode='fan_out', a=math.sqrt(5))\n",
    "        if self.bias is not None:\n",
    "            _, fan_out = nn.init._calculate_fan_in_and_fan_out(self.weight)\n",
    "            bound = 1 / math.sqrt(fan_out)\n",
    "            nn.init.uniform_(self.bias, -bound, bound)\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.bias is None:\n",
    "            if input.is_sparse:\n",
    "                res = torch.sparse.mm(input, self.weight)\n",
    "            else:\n",
    "                res = input.matmul(self.weight)\n",
    "        else:\n",
    "            if input.is_sparse:\n",
    "                res = torch.sparse.addmm(self.bias.expand(input.shape[0], -1), input, self.weight)\n",
    "            else:\n",
    "                res = torch.addmm(self.bias, input, self.weight)\n",
    "        return res\n",
    "\n",
    "    def extra_repr(self):\n",
    "        return 'in_features={}, out_features={}, bias={}'.format(\n",
    "                self.in_features, self.out_features, self.bias is not None)\n",
    "        \n",
    "class MixedDropout(nn.Module):\n",
    "    def __init__(self, p):\n",
    "        super(MixedDropout, self).__init__()\n",
    "        self.dense_dropout = nn.Dropout(p)\n",
    "        self.sparse_dropout = SparseDropout(p)\n",
    "\n",
    "    def forward(self, input):\n",
    "        if input.is_sparse:\n",
    "            return self.sparse_dropout(input)\n",
    "        else:\n",
    "            return self.dense_dropout(input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "similar-prospect",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:16: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:16: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<ipython-input-45-5962d3388f97>:16: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  if self.drop_prob is 0:\n"
     ]
    }
   ],
   "source": [
    "class APPNP(nn.Module):\n",
    "    def __init__(self, nfeat, nclass, device):\n",
    "        super(APPNP, self).__init__()\n",
    "        self.lr=0.01\n",
    "        self.drop_prob = 0.5\n",
    "        self.weight_decay=5e-6\n",
    "        self.nfeat = nfeat\n",
    "        self.nclass = nclass\n",
    "        self.train_iters = 200\n",
    "        self.alpha = 0.1\n",
    "        self.niter = 10\n",
    "        self.hiddenunits = [64]\n",
    "        bias = False\n",
    "        \n",
    "        # dropout\n",
    "        if self.drop_prob is 0:\n",
    "            self.dropout = lambda x: x\n",
    "        else:\n",
    "            self.dropout = MixedDropout(self.drop_prob)\n",
    "        self.act_fn = nn.ReLU()\n",
    "        assert device is not None, \"Please specify 'device'!\"\n",
    "        \n",
    "        # 线性变换\n",
    "        fcs = [MixedLinear(nfeat, self.hiddenunits[0], bias=bias)] #wx+b\n",
    "        for i in range(1, len(self.hiddenunits)):\n",
    "            fcs.append(nn.Linear(self.hiddenunits[i - 1], self.hiddenunits[i], bias=bias)) \n",
    "        fcs.append(nn.Linear(self.hiddenunits[-1], nclass, bias=bias))# len(hiddenunits)层的wx+b\n",
    "        self.fcs = nn.ModuleList(fcs)\n",
    "        \n",
    "        self.device = device\n",
    "        self.reg_params = list(self.fcs[0].parameters())\n",
    "        \n",
    "    def _transform_features(self, features):#多层MLP\n",
    "        layer_inner = self.act_fn(self.fcs[0](self.dropout(features))) \n",
    "        for fc in self.fcs[1:-1]:\n",
    "            layer_inner = self.act_fn(fc(layer_inner))\n",
    "        res = self.fcs[-1](self.dropout(layer_inner))\n",
    "        return res\n",
    "\n",
    "    def initialize(self):\n",
    "        for fc in self.fcs:\n",
    "            fc.reset_parameters()\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        x = self._transform_features(x)\n",
    "        z = x\n",
    "        adj = self.dropout(adj)#??\n",
    "        for _ in range(self.niter):\n",
    "            z = adj @ z + self.alpha * x\n",
    "        return F.log_softmax(z, dim=-1)\n",
    "        \n",
    "    def fit(self, features, adj, labels, idx_train, idx_val=None, normalize=True, initialize=True, verbose=False, patience=30, **kwargs):\n",
    "        if initialize:\n",
    "            self.initialize()\n",
    "            \n",
    "        if type(adj) is not torch.Tensor:\n",
    "            features, adj, labels = utils.to_tensor(features, adj, labels, device=self.device)\n",
    "        else:\n",
    "            features = features.to(self.device)\n",
    "            adj = adj.to(self.device)\n",
    "            labels = labels.to(self.device)\n",
    "\n",
    "        if normalize:\n",
    "            if utils.is_sparse_tensor(adj):\n",
    "                adj_norm = utils.normalize_adj_tensor(adj, sparse=True)\n",
    "            else:\n",
    "                adj_norm = utils.normalize_adj_tensor(adj)\n",
    "        else:\n",
    "            self.adj_norm = adj\n",
    "\n",
    "        self.adj_norm = adj_norm\n",
    "        self.features = features\n",
    "        self.labels = labels\n",
    "\n",
    "        optimizer = optim.Adam(self.parameters(), lr=self.lr, weight_decay=self.weight_decay)\n",
    "\n",
    "        early_stopping = patience\n",
    "        best_loss_val = 30\n",
    "        \n",
    "\n",
    "        for i in range(self.train_iters):\n",
    "            self.train()\n",
    "            optimizer.zero_grad()\n",
    "            output = self.forward(features, adj_norm)\n",
    "\n",
    "            loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
    "            loss_train.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            if verbose and i % 10 == 0:\n",
    "                print('Epoch {}, training loss: {}'.format(i, loss_train.item()))\n",
    "\n",
    "            self.eval()\n",
    "            output = self.forward(features, adj_norm)\n",
    "            loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
    "\n",
    "            if best_loss_val > loss_val:\n",
    "                best_loss_val = loss_val\n",
    "                self.output = output\n",
    "                weights = deepcopy(self.state_dict())\n",
    "                patience = early_stopping\n",
    "            else:\n",
    "                patience -= 1\n",
    "            if i > early_stopping and patience <= 0:\n",
    "                break\n",
    "\n",
    "        if verbose:\n",
    "             print('=== early stopping at {0}, loss_val = {1} ==='.format(i, best_loss_val) )\n",
    "        self.load_state_dict(weights)\n",
    "    def test(self, idx_test):\n",
    "        self.eval()\n",
    "        output = self.predict()\n",
    "        # output = self.output\n",
    "        loss_test = F.nll_loss(output[idx_test], self.labels[idx_test])\n",
    "        acc_test = utils.accuracy(output[idx_test], self.labels[idx_test])\n",
    "        print(\"Test set results:\",\n",
    "              \"loss= {:.4f}\".format(loss_test.item()),\n",
    "              \"accuracy= {:.4f}\".format(acc_test.item()))\n",
    "        return acc_test\n",
    "\n",
    "    def predict(self, features=None, adj=None):\n",
    "        self.eval()\n",
    "        if features is None and adj is None:\n",
    "            return self.forward(self.features, self.adj_norm)\n",
    "        else:\n",
    "            if type(adj) is not torch.Tensor:\n",
    "                features, adj = utils.to_tensor(features, adj, device=self.device)\n",
    "\n",
    "            if utils.is_sparse_tensor(adj):\n",
    "                adj_norm = utils.normalize_adj_tensor(adj, sparse=True)\n",
    "            else:\n",
    "                adj_norm = utils.normalize_adj_tensor(adj)\n",
    "            return self.forward(features, adj_norm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "harmful-forest",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, training loss: 1.952162742614746\n",
      "Epoch 10, training loss: 1.6863492727279663\n",
      "Epoch 20, training loss: 1.407976746559143\n",
      "Epoch 30, training loss: 1.2235996723175049\n",
      "Epoch 40, training loss: 1.1078435182571411\n",
      "Epoch 50, training loss: 0.8753387331962585\n",
      "Epoch 60, training loss: 0.9004809856414795\n",
      "Epoch 70, training loss: 0.8362159132957458\n",
      "Epoch 80, training loss: 0.6884917616844177\n",
      "Epoch 90, training loss: 0.7960319519042969\n",
      "Epoch 100, training loss: 0.6922302842140198\n",
      "Epoch 110, training loss: 0.7657747268676758\n",
      "Epoch 120, training loss: 0.8878371715545654\n",
      "Epoch 130, training loss: 0.6054678559303284\n",
      "Epoch 140, training loss: 0.6819593906402588\n",
      "Epoch 150, training loss: 0.5922753810882568\n",
      "Epoch 160, training loss: 0.6309047341346741\n",
      "Epoch 170, training loss: 0.8365095257759094\n",
      "Epoch 180, training loss: 0.5933727025985718\n",
      "Epoch 190, training loss: 0.579773485660553\n",
      "=== early stopping at 192, loss_val = 0.6070668697357178 ===\n",
      "Test set results: loss= 0.5996 accuracy= 0.8400\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(0.8400, device='cuda:0', dtype=torch.float64)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 3\n",
    "\n",
    "dataset = 'cora'\n",
    "\n",
    "config = Config(dataset)\n",
    "config.device = 0\n",
    "device = config.device\n",
    "\n",
    "seed = 19\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)\n",
    "random.seed(seed)\n",
    "\n",
    "data = Data(dataset, dataset+'_ft_norm')\n",
    "\n",
    "appnp = APPNP(nfeat=data.features.shape[1], nclass=data.labels.max().item() + 1, device=device)\n",
    "appnp = appnp.to(device)\n",
    "appnp.fit(data.features_norm.todense(), data.adj.todense(), \\\n",
    "        data.labels, data.idx_train, data.idx_val, normalize=True, verbose=True)\n",
    "appnp.test(data.idx_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surface-australian",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
